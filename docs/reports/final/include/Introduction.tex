% CREATED BY DAVID FRISK, 2016
\chapter{Introduction}
\section{Background About Dependent Types}
Dependent type theory has lent much of its power to the proof-assistant systems like Coq \cite{huet1997coq}, Lean \cite{de2015lean}, and functional programming languages like Agda \cite{norell2008dependently} and Idris \cite{brady2013idris}, and contributed much to their success. Essentially, dependent types are types that depend on \emph{values} of other types. As a simple example, consider the type that represents vectors of length $n$ comprising of elements of type $A$, which can be expressed as a dependent type ($\texttt{vec}\; A\; n$). Readers may easily recall that in imperative languages such as C/C++ or Java, there are array types which depend on the type of their elements, but not types that depend on values of other types. More formally, suppose we have defined a function which to an arbitrary object $x$ of type $A$ assigns a type $B(x)$, then the Cartesian product $(\Pi x \in A)B(x)$ is a type, namely the type of functions which take an arbitrary object $x$ of type $A$ into an object of type $B(x)$.

The advantage of having a strong typed system built into a language lies in the fact that well typed programs exclude a large portion of run-time errors than those without or with weak type systems. Just as the famous saying puts it “well-type programs cannot ‘go wrong’” [16]. It is in this sense that we say languages with dependent types are guaranteed with the highest level of correctness and precision, which makes them a natural option for building proof assistant systems.

\section{Issues with Dependent Types}\label{chapter:intro:issue}
The downside of dependent type systems lies in the difficulties of implementation, among which is the notable problem about checking the \textbf{convertibility} of terms: given two terms $A,B$, decide whether they are equal or not. Checking the convertibility of terms that represent types is a routinely performed task by the type checker of any typed language, thus is vital for its performance. In a simple typed language, convertibility checking is done by simply checking the syntactic identity of the symbols of the types. For example, in Java, a primitive type \emph{int} equals only to itself, nothing more. This is because types in Java are not computable \footnote{Technically speaking, the type of an object in Java can be retrieved by the Java \emph{reflection} mechanism and presented in the form of another object, thus subject to computation. Here, we stress on the fact that a type as a term is not computable on the syntactic level, e.g. being passed as an argument to a function.}:there's no way for other terms in Java be reduced to the term \emph{int}. In a dependently typed language, however, the problem is more complex since a type may contain terms of any expression, deciding the convertibility of types in this case entails evaluation of any terms, which requires much more computation.

One common approach to deciding the convertibility of terms in dependent type theory, whenever the property of confluence holds, is \textit{normalization by evaluation} (NbE) \cite{berger1998normalization}, which reduces terms to their canonical representation for comparison. This method, however, does not scale to large theories for various reasons, among which:
\begin{itemize}
\item Producing the normal form may require more reduction steps than necessary. For example, in proving $(1 + 1) ^ {10} = 2 ^{(5 + 5)}$, it is easier if we can prove $1 + 1 == 2$ and $5 + 5 == 10$ instead of having to reduce both sides to 1024 by the definition of exponentiation.
\item As the number of definition grows, the size of terms by expanding a definition can grow very quickly. For example, the inductive definition $x_n := (x_{n-1}, x_{n-1})$ makes the normal form of $x_{n}$ grow exponentially.
\end{itemize}

In this project, we shall focus on the first issue, that is, how to perform as few constant expansions as possible when deciding the convertibility of two terms in a dependently typed language. 

\section{Aim of the Project}\label{chapter:intro:aim}
The first aim of the project is to study how to present \emph{definitions} in a dependently typed language, more precisely, how to do type checking in the presence of \emph{definitions}? We hope that the definitions of constants could be expanded as few times as possible during the type checking process. We claim that a good definition mechanism can help improve the performance of a language that is based on dependent type theory. We will justify this claim later by giving an analysis to the example above. Before that, we shall first make it clear for the reader this question: what exactly is the problem of definition and why is it important?

A definition in a dependently typed language is a declaration of the form $x : A = B$, meaning that $x$ is a constant of type $A$, defined as $B$. The problem with definitions is not about how a constant should be declared, but how it should be evaluated. Evaluation, or reduction, in dependent type theory has its concept originated in \emph{$\lambda$-calculus} \cite{barendregt1984lambda}. There, a term of the form $(\lambda x . M) N$ can be evaluated (or reduced) to the form $M[x := N]$, meaning the substitution of $N$ for the free occurrences of $x$ in $M$\footnote{There is a problem of the capture of free variables which we will not elaborate here. Curious and uninformed readers are encouraged to read articles introducing \emph{$\lambda$-calculus.}, especially the 1984 book \emph{The lambda calculus: its syntax and semantics} by Hendrik Pieter Barendregt}. In dependent type theory, however, different evaluation strategies can have huge difference regarding the efficiency of evaluation.  

For example, if we define the exponentiation function on natural numbers as
\begin{align*}
  \texttt{expo} &: \texttt{Nat} \to \texttt{Nat} \to \texttt{Nat} \\
  \texttt{expo} &\;\; \_\;\; 0 = 1 \\
  \texttt{expo} &\;\; n \;\; m = n * (\texttt{expo} \;\; n \;\; (m - 1))
\end{align*}
where \texttt{Nat} is the type of natural number and ($*$) is the operator of multiplication. Then when we try to prove the convertibility of two terms: $(1 + 1)^{10}$ and $2 ^ {(5+5)}$, instead of unfolding the definition of \texttt{expo} multiple times, we keep the constant \texttt{expo} \textbf{locked} and only reduce both sides to the term $\texttt{expo}\;2\;10$. Then by showing that they can be reduced to a common term(having the same symbolic representation), we proved their equality with much less computation. Locking a constant has the effect of turning its definition $x : A = B$ into a declaration $x : A$, so that the type information can still be used in the type checking process, whereas the definition is erased so that the constant cannot be reduced further. Our definition mechanism could be seen as a simple computation control technique.

The second aim of the project is to extend the language with a module system based on the idea `segment' originated from the work AUTOMATH \cite{de1994survey}. AUTOMATH was initiated by Dutch mathematician N.G.de Bruijn where special languages were developed to express concepts in mathematics. The ingenuity of the work is that the correctness(in the sense of mathematical deduction) of a text written in AUTOMATH could be checked by a computer program mechanically. The idea of `segment' was introduced as a facility of abbreviation in AUTOMATH for easier reference to patterns of strings that usually appear as part of a larger formula. For example, using the notations from $\lambda$-calculus, consider a function abstraction of the form $\lambda x_1\,.\,\lambda x_2\,\dots\lambda x_n\,.\,\phi$, where $x_1,\dots,x_n$ are bound variables. If $n$ is large and this pattern of abstraction appears multiple times in a larger formula, then the abstraction part over $x_1,\dots,x_n$, i.e., $\lambda x_1\,.\,\lambda x_2\,\dots\lambda x_n$ without the body $\phi$ could be taken as a `segment' in AUTOMATH.

We use the idea `segment' to build a module system that shares similarities with those of the common programming languages like Java and Haskell, but less expressive. Again, our aim is not to increase the expressiveness of the language but to study how the definition mechanism built in the first step should be adjusted to accommodate the concept of `namespaces' introduced by the module system. The result could be used as an evidence for the scalability of our definition mechanism.

\section{Organization of This Paper}
This paper is organized as follows: In chapter \ref{chapter:theory}, we first describe the common pitfalls one should avoid in the implementation of a dependently typed language. Based on the pitfalls, we put forward the principles that guide us through the implementation of our system. The rest of the chapter \ref{chapter:theory} devotes to a detailed description of the basic form of our language: a pure $\lambda$-calculus extended with a universe of small types($U$), dependent types and definitions. We present in detail its syntax, semantics, type checking rules and the definition mechanism plus some related important operations. In chapter \ref{chapter:extension}, we present an extension of our language with the introduction of a module system. In a same way as in chapter \ref{chapter:theory}, we describe the extended syntax, semantics, type checking rules and the related important operations. Particularly, we draw our attention to the comparison with chapter \ref{chapter:theory} where operations related with the definition mechanism need to be adjusted for the `namespaces' concept brought in by the module system. Chapter \ref{chapter:results} presents as results a REPL program that could be used to load and type check a source file and to experiment with the definition mechanism. Chapter \ref{chapter:conclusion} concludes the paper with a short review of the project.

\section{Limitations of the Project}
The limitations of our work come into three aspects: expressiveness, scope and metatheory.
\begin{enumerate}
\item \textbf{Expressiveness/Usability:} We try to keep the syntax of the language as simple as possible in order to focus on the study of a definition mechanism. This practice inevitably affects the expressiveness and usability of our language: As has been mentioned, there is no syntax in support for creating new data types, defining recursive functions or doing pattern match on expressions; Further, there is a lack of basic input/output functionalities(one cannot even print a ``Hello,world!'' on the screen); Besides, because we track the names of constants in a linear manner as an approach to the name collision problem (see example \ref{theory:exa2} in section \ref{theory:subtleties}) and enforce the policy that declaration of names must not collide with the names in the current context, the common programming language feature \emph{variable shadowing} does not exist in our language; Lastly, the result of the project is only a REPL that incorporates a type checker(along with a lexer and a parser which are automatically generated by the BNF convertor) and an interpreter, the former to load and type check a source file and the latter to evaluate expressions. There is no support in the view of a compiler that generates executable machine code(of course for a language worthy of executable program compilation, it must have basic data types and I/O operations).
\item \textbf{Scope:} The definition mechanism we established is not meant to be the most effective or applicable universally to different kinds of dependently typed languages. This means if we were to develop a more practical dependently typed language, then (1) the definition mechanism along with the type checking algorithm must be modified, perhaps dramatically, to accommodate the semantics brought in by the new language features; (2) the definition mechanism is subject to improvement by utilizing other computation control techniques; (3) the type checking algorithm is subject to improvement by optimization.
\item \textbf{Metatheory:} The properties of our language as a type theory, such as the decidability of our type checking algorithm, will not be covered in this paper. The connections between our work and other logical systems from type theory will also be ignored. Although most of the ideas used in this work originated from the work AUTOMATH, a comprehensive exposition of AUTOMATH and the connections between these two works will not be drawn here. A careful study of the metatheory of this project and its links with other logical systems in a broader picture of type theory could be viewed as a direction for the future work.
\end{enumerate}
